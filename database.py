"""
æ•°æ®åˆ†æå¸ˆæ’ä»¶ - æ•°æ®åº“ç®¡ç†æ¨¡å—

æä¾›å¼‚æ­¥æ•°æ®åº“æ“ä½œã€æ•°æ®å­˜å‚¨å’ŒæŸ¥è¯¢åŠŸèƒ½
"""

import aiosqlite
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any
from pathlib import Path

from astrbot.api.event import AstrMessageEvent
from astrbot.api import logger
import jieba

from .models import (
    MessageData, UserStats, GroupStats, TopicKeyword,
    ActivityAnalysisData, UserAnalysisData, TopicsAnalysisData,
    DatabaseConstants as DB, TimePeriod
)
from .privacy import PrivacyFilter


class DatabaseManager:
    """
    /// æ•°æ®åº“ç®¡ç†å™¨
    /// è´Ÿè´£æ‰€æœ‰æ•°æ®åº“æ“ä½œï¼ŒåŒ…æ‹¬æ•°æ®å­˜å‚¨ã€æŸ¥è¯¢ã€ç»Ÿè®¡å’Œç»´æŠ¤
    /// ä½¿ç”¨å¼‚æ­¥SQLiteç¡®ä¿é«˜æ€§èƒ½å’Œå¹¶å‘å®‰å…¨
    """
    
    def __init__(self, db_path: str):
        """
        /// åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        /// @param db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = db_path
        self.is_initialized = False
        
        # åˆ›å»ºæ•°æ®åº“ç›®å½•
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–: {db_path}")
    
    async def initialize(self):
        """
        /// åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„å’Œç´¢å¼•
        /// åˆ›å»ºæ‰€æœ‰å¿…è¦çš„è¡¨å’Œä¼˜åŒ–ç´¢å¼•
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # å¯ç”¨WALæ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
                await db.execute('PRAGMA journal_mode=WAL')
                await db.execute('PRAGMA synchronous=NORMAL')
                await db.execute('PRAGMA cache_size=10000')
                
                # åˆ›å»ºæ¶ˆæ¯è®°å½•è¡¨
                await self._create_messages_table(db)
                
                # åˆ›å»ºç”¨æˆ·ç»Ÿè®¡è¡¨
                await self._create_user_stats_table(db)
                
                # åˆ›å»ºç¾¤ç»„ç»Ÿè®¡è¡¨
                await self._create_group_stats_table(db)
                
                # åˆ›å»ºè¯é¢˜å…³é”®è¯è¡¨
                await self._create_topic_keywords_table(db)
                
                # åˆ›å»ºåˆ†æç¼“å­˜è¡¨
                await self._create_analysis_cache_table(db)
                
                # åˆ›å»ºç´¢å¼•
                await self._create_indexes(db)
                
                await db.commit()
                
                # Phase 2 æ–°å¢ï¼šè¯äº‘å†å²è¡¨
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS wordcloud_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        group_id TEXT NOT NULL,
                        time_range TEXT NOT NULL,
                        word_data TEXT NOT NULL,  -- JSONæ ¼å¼å­˜å‚¨è¯é¢‘æ•°æ®
                        style_name TEXT NOT NULL,
                        total_words INTEGER DEFAULT 0,
                        file_path TEXT,
                        metadata TEXT,  -- JSONæ ¼å¼å­˜å‚¨å…ƒæ•°æ®
                        generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # åˆ›å»ºè¯äº‘å†å²ç´¢å¼•
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_wordcloud_group_time 
                    ON wordcloud_history(group_id, time_range, generated_at DESC)
                """)
                
                # Phase 3 æ–°å¢ï¼šç”¨æˆ·ç”»åƒè¡¨
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS user_portraits (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        user_id TEXT NOT NULL,
                        group_id TEXT NOT NULL,
                        nickname TEXT,
                        portrait_data TEXT NOT NULL,  -- JSONæ ¼å¼å­˜å‚¨ç”»åƒæ•°æ®
                        analysis_depth TEXT NOT NULL,
                        data_quality_score REAL,
                        analysis_duration REAL,
                        generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        UNIQUE(user_id, group_id, analysis_depth)
                    )
                """)
                
                # åˆ›å»ºç”¨æˆ·ç”»åƒç´¢å¼•
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_user_portraits_user_group 
                    ON user_portraits(user_id, group_id, generated_at DESC)
                """)
                
                # ç”»åƒåˆ†æå†å²è¡¨
                await db.execute("""
                    CREATE TABLE IF NOT EXISTS portrait_analysis_history (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        user_id TEXT NOT NULL,
                        group_id TEXT NOT NULL,
                        analysis_type TEXT NOT NULL,  -- 'portrait', 'comparison'
                        analysis_depth TEXT NOT NULL,
                        result_summary TEXT,
                        file_paths TEXT,  -- JSONæ ¼å¼å­˜å‚¨ç›¸å…³æ–‡ä»¶è·¯å¾„
                        metadata TEXT,    -- JSONæ ¼å¼å­˜å‚¨å…ƒæ•°æ®
                        analysis_time REAL,
                        generated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                
                # åˆ›å»ºå†å²ç´¢å¼•
                await db.execute("""
                    CREATE INDEX IF NOT EXISTS idx_portrait_history_user 
                    ON portrait_analysis_history(user_id, group_id, generated_at DESC)
                """)
                
                await db.commit()
                
            self.is_initialized = True
            logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ (åŒ…å«è¯äº‘å†å²å’Œç”¨æˆ·ç”»åƒåŠŸèƒ½)")
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _create_messages_table(self, db: aiosqlite.Connection):
        """åˆ›å»ºæ¶ˆæ¯è®°å½•è¡¨"""
        await db.execute(f'''
            CREATE TABLE IF NOT EXISTS {DB.TABLE_MESSAGES} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                message_id TEXT UNIQUE,
                user_id TEXT NOT NULL,
                group_id TEXT,
                platform TEXT NOT NULL,
                content_hash TEXT,
                message_type TEXT DEFAULT '{DB.MESSAGE_TYPE_TEXT}',
                timestamp DATETIME NOT NULL,
                word_count INTEGER DEFAULT {DB.DEFAULT_WORD_COUNT},
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    
    async def _create_user_stats_table(self, db: aiosqlite.Connection):
        """åˆ›å»ºç”¨æˆ·ç»Ÿè®¡è¡¨"""
        await db.execute(f'''
            CREATE TABLE IF NOT EXISTS {DB.TABLE_USER_STATS} (
                user_id TEXT PRIMARY KEY,
                username TEXT,
                total_messages INTEGER DEFAULT 0,
                total_words INTEGER DEFAULT 0,
                first_seen DATETIME,
                last_seen DATETIME,
                active_days INTEGER DEFAULT 0,
                avg_words_per_msg REAL DEFAULT 0,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    
    async def _create_group_stats_table(self, db: aiosqlite.Connection):
        """åˆ›å»ºç¾¤ç»„ç»Ÿè®¡è¡¨"""
        await db.execute(f'''
            CREATE TABLE IF NOT EXISTS {DB.TABLE_GROUP_STATS} (
                group_id TEXT PRIMARY KEY,
                group_name TEXT,
                total_messages INTEGER DEFAULT 0,
                total_members INTEGER DEFAULT 0,
                peak_hour INTEGER DEFAULT 0,
                most_active_day TEXT,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    
    async def _create_topic_keywords_table(self, db: aiosqlite.Connection):
        """åˆ›å»ºè¯é¢˜å…³é”®è¯è¡¨"""
        await db.execute(f'''
            CREATE TABLE IF NOT EXISTS {DB.TABLE_TOPIC_KEYWORDS} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                keyword TEXT NOT NULL,
                group_id TEXT,
                frequency INTEGER DEFAULT {DB.DEFAULT_FREQUENCY},
                last_mentioned DATETIME,
                sentiment_score REAL DEFAULT {DB.DEFAULT_SENTIMENT},
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    
    async def _create_analysis_cache_table(self, db: aiosqlite.Connection):
        """åˆ›å»ºåˆ†æç¼“å­˜è¡¨"""
        await db.execute(f'''
            CREATE TABLE IF NOT EXISTS {DB.TABLE_ANALYSIS_CACHE} (
                cache_key TEXT PRIMARY KEY,
                analysis_type TEXT NOT NULL,
                result_data TEXT,
                expires_at DATETIME NOT NULL,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
    
    async def _create_indexes(self, db: aiosqlite.Connection):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        indexes = [
            f'CREATE INDEX IF NOT EXISTS {DB.IDX_MESSAGES_TIMESTAMP} ON {DB.TABLE_MESSAGES}(timestamp)',
            f'CREATE INDEX IF NOT EXISTS {DB.IDX_MESSAGES_GROUP_ID} ON {DB.TABLE_MESSAGES}(group_id)',
            f'CREATE INDEX IF NOT EXISTS {DB.IDX_MESSAGES_USER_ID} ON {DB.TABLE_MESSAGES}(user_id)',
            f'CREATE INDEX IF NOT EXISTS {DB.IDX_TOPIC_KEYWORDS_GROUP_ID} ON {DB.TABLE_TOPIC_KEYWORDS}(group_id)',
            f'CREATE INDEX IF NOT EXISTS idx_user_stats_updated ON {DB.TABLE_USER_STATS}(updated_at)',
            f'CREATE INDEX IF NOT EXISTS idx_topic_keywords_frequency ON {DB.TABLE_TOPIC_KEYWORDS}(frequency DESC)',
            f'CREATE INDEX IF NOT EXISTS idx_analysis_cache_expires ON {DB.TABLE_ANALYSIS_CACHE}(expires_at)'
        ]
        
        for index_sql in indexes:
            await db.execute(index_sql)
    
    async def collect_message(self, event: AstrMessageEvent, privacy_filter: PrivacyFilter):
        """
        /// æ”¶é›†æ¶ˆæ¯æ•°æ®
        /// @param event: AstrBotæ¶ˆæ¯äº‹ä»¶
        /// @param privacy_filter: éšç§è¿‡æ»¤å™¨
        """
        try:
            # æå–æ¶ˆæ¯åŸºç¡€ä¿¡æ¯
            message_data = self._extract_message_data(event, privacy_filter)
            
            async with aiosqlite.connect(self.db_path) as db:
                # æ’å…¥æ¶ˆæ¯è®°å½•
                await self._insert_message(db, message_data)
                
                # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡
                await self._update_user_stats(db, message_data)
                
                # æ›´æ–°ç¾¤ç»„ç»Ÿè®¡
                if message_data.group_id:
                    await self._update_group_stats(db, message_data)
                
                # æå–å’Œå­˜å‚¨å…³é”®è¯
                if message_data.message_type == DB.MESSAGE_TYPE_TEXT and message_data.word_count > 2:
                    await self._extract_and_store_keywords(db, event.message_str, message_data)
                
                await db.commit()
                
        except Exception as e:
            logger.error(f"æ¶ˆæ¯æ”¶é›†å¤±è´¥: {e}")
    
    def _extract_message_data(self, event: AstrMessageEvent, privacy_filter: PrivacyFilter) -> MessageData:
        """æå–æ¶ˆæ¯æ•°æ®"""
        # ç”Ÿæˆæ¶ˆæ¯ID
        message_id = getattr(event.message_obj, 'message_id', 
                           f"{event.get_sender_id()}_{int(time.time())}")
        
        # è·å–åŸºç¡€ä¿¡æ¯
        user_id = event.get_sender_id()
        group_id = event.get_group_id()
        platform = event.get_platform_name()
        content = event.message_str or ""
        
        # è¿‡æ»¤å†…å®¹
        filtered_content = privacy_filter.filter_content(content)
        word_count = len(content) if content else 0
        
        # ç¡®å®šæ¶ˆæ¯ç±»å‹
        message_type = self._determine_message_type(event)
        
        return MessageData(
            message_id=message_id,
            user_id=user_id,
            group_id=group_id,
            platform=platform,
            content_hash=filtered_content,
            message_type=message_type,
            timestamp=datetime.now(),
            word_count=word_count
        )
    
    def _determine_message_type(self, event: AstrMessageEvent) -> str:
        """ç¡®å®šæ¶ˆæ¯ç±»å‹"""
        if hasattr(event.message_obj, 'message') and event.message_obj.message:
            for msg_seg in event.message_obj.message:
                if hasattr(msg_seg, 'type'):
                    if msg_seg.type in ['image', 'voice', 'video', 'file']:
                        return msg_seg.type
        return DB.MESSAGE_TYPE_TEXT
    
    async def _insert_message(self, db: aiosqlite.Connection, message_data: MessageData):
        """æ’å…¥æ¶ˆæ¯è®°å½•"""
        await db.execute(f'''
            INSERT OR REPLACE INTO {DB.TABLE_MESSAGES} 
            (message_id, user_id, group_id, platform, content_hash, message_type, timestamp, word_count)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            message_data.message_id,
            message_data.user_id,
            message_data.group_id,
            message_data.platform,
            message_data.content_hash,
            message_data.message_type,
            message_data.timestamp,
            message_data.word_count
        ))
    
    async def _update_user_stats(self, db: aiosqlite.Connection, message_data: MessageData):
        """æ›´æ–°ç”¨æˆ·ç»Ÿè®¡"""
        await db.execute(f'''
            INSERT OR REPLACE INTO {DB.TABLE_USER_STATS} 
            (user_id, username, total_messages, total_words, first_seen, last_seen, updated_at)
            VALUES (
                ?, '', 
                COALESCE((SELECT total_messages FROM {DB.TABLE_USER_STATS} WHERE user_id = ?), 0) + 1,
                COALESCE((SELECT total_words FROM {DB.TABLE_USER_STATS} WHERE user_id = ?), 0) + ?,
                COALESCE((SELECT first_seen FROM {DB.TABLE_USER_STATS} WHERE user_id = ?), ?),
                ?,
                ?
            )
        ''', (
            message_data.user_id, message_data.user_id, message_data.user_id,
            message_data.word_count, message_data.user_id, message_data.timestamp,
            message_data.timestamp, message_data.timestamp
        ))
    
    async def _update_group_stats(self, db: aiosqlite.Connection, message_data: MessageData):
        """æ›´æ–°ç¾¤ç»„ç»Ÿè®¡"""
        await db.execute(f'''
            INSERT OR REPLACE INTO {DB.TABLE_GROUP_STATS} 
            (group_id, total_messages, updated_at)
            VALUES (
                ?, 
                COALESCE((SELECT total_messages FROM {DB.TABLE_GROUP_STATS} WHERE group_id = ?), 0) + 1,
                ?
            )
        ''', (message_data.group_id, message_data.group_id, message_data.timestamp))
    
    async def _extract_and_store_keywords(self, db: aiosqlite.Connection, content: str, message_data: MessageData):
        """æå–å¹¶å­˜å‚¨å…³é”®è¯"""
        try:
            # ä½¿ç”¨jiebaåˆ†è¯
            words = jieba.cut(content)
            keywords = [word.strip() for word in words 
                       if len(word.strip()) > 1 and word.strip().isalpha()]
            
            # å­˜å‚¨å…³é”®è¯ï¼ˆé™åˆ¶æ•°é‡é¿å…åƒåœ¾æ•°æ®ï¼‰
            for keyword in keywords[:10]:
                await db.execute(f'''
                    INSERT OR REPLACE INTO {DB.TABLE_TOPIC_KEYWORDS} 
                    (keyword, group_id, frequency, last_mentioned)
                    VALUES (
                        ?, ?,
                        COALESCE((SELECT frequency FROM {DB.TABLE_TOPIC_KEYWORDS} WHERE keyword = ? AND group_id = ?), 0) + 1,
                        ?
                    )
                ''', (keyword, message_data.group_id, keyword, message_data.group_id, message_data.timestamp))
                
        except Exception as e:
            logger.error(f"å…³é”®è¯æå–å¤±è´¥: {e}")
    
    async def get_group_quick_stats(self, group_id: str) -> Dict:
        """
        /// è·å–ç¾¤ç»„å¿«é€Ÿç»Ÿè®¡
        /// @param group_id: ç¾¤ç»„ID
        /// @return: ç»Ÿè®¡æ•°æ®å­—å…¸
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # åŸºç¡€ç»Ÿè®¡
                cursor = await db.execute(f'''
                    SELECT 
                        COUNT(*) as total_messages,
                        COUNT(DISTINCT user_id) as active_users,
                        AVG(word_count) as avg_length,
                        MIN(timestamp) as first_message,
                        MAX(timestamp) as last_message
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = ?
                ''', (group_id,))
                
                row = await cursor.fetchone()
                if not row or row[0] == 0:
                    return {}
                
                # è®¡ç®—æ•°æ®æ”¶é›†å¤©æ•°
                first_date = datetime.fromisoformat(row[3]) if row[3] else datetime.now()
                data_days = (datetime.now() - first_date).days + 1
                
                # è·å–æœ€æ´»è·ƒæ—¶æ®µ
                cursor = await db.execute(f'''
                    SELECT strftime('%H', timestamp) as hour, COUNT(*) as count
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = ?
                    GROUP BY hour
                    ORDER BY count DESC
                    LIMIT 1
                ''', (group_id,))
                
                peak_hour_row = await cursor.fetchone()
                peak_hour = peak_hour_row[0] if peak_hour_row else 'N/A'
                
                return {
                    'total_messages': row[0],
                    'active_users': row[1],
                    'avg_message_length': row[2] or 0,
                    'data_days': data_days,
                    'peak_hour': peak_hour
                }
                
        except Exception as e:
            logger.error(f"å¿«é€Ÿç»Ÿè®¡æŸ¥è¯¢å¤±è´¥: {e}")
            return {}
    
    async def get_activity_analysis(self, group_id: str, period: str) -> Optional[ActivityAnalysisData]:
        """
        /// è·å–æ´»è·ƒåº¦åˆ†ææ•°æ®
        /// @param group_id: ç¾¤ç»„ID
        /// @param period: æ—¶é—´å‘¨æœŸ
        /// @return: æ´»è·ƒåº¦åˆ†ææ•°æ®
        """
        try:
            start_date = self._calculate_start_date(period)
            
            async with aiosqlite.connect(self.db_path) as db:
                # è·å–æ¯æ—¥æ•°æ®
                cursor = await db.execute(f'''
                    SELECT DATE(timestamp) as date, COUNT(*) as daily_count
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = ? AND timestamp >= ?
                    GROUP BY date
                    ORDER BY date
                ''', (group_id, start_date))
                
                daily_data = await cursor.fetchall()
                if not daily_data:
                    return None
                
                # åŸºç¡€ç»Ÿè®¡
                total_messages = sum(row[1] for row in daily_data)
                
                cursor = await db.execute(f'''
                    SELECT COUNT(DISTINCT user_id) as active_users
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = ? AND timestamp >= ?
                ''', (group_id, start_date))
                
                active_users = (await cursor.fetchone())[0]
                
                # æœ€æ´»è·ƒæ—¶æ®µ
                cursor = await db.execute(f'''
                    SELECT strftime('%H', timestamp) as hour, COUNT(*) as count
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = ? AND timestamp >= ?
                    GROUP BY hour
                    ORDER BY count DESC
                    LIMIT 1
                ''', (group_id, start_date))
                
                peak_hour_row = await cursor.fetchone()
                peak_hour = peak_hour_row[0] if peak_hour_row else 'N/A'
                
                # è®¡ç®—è¶‹åŠ¿
                avg_daily_messages = total_messages / max(1, len(daily_data))
                growth_rate = self._calculate_growth_rate(daily_data)
                trend_description = self._generate_trend_description(growth_rate)
                
                return ActivityAnalysisData(
                    total_messages=total_messages,
                    active_users=active_users,
                    avg_daily_messages=avg_daily_messages,
                    growth_rate=growth_rate,
                    peak_hour=peak_hour,
                    peak_day=daily_data[-1][0] if daily_data else 'N/A',
                    trend_description=trend_description,
                    daily_data=daily_data,
                    timespan_days=len(daily_data)
                )
                
        except Exception as e:
            logger.error(f"æ´»è·ƒåº¦åˆ†æå¤±è´¥: {e}")
            return None
    
    async def get_user_analysis(self, user_id: str, period: str) -> Optional[UserAnalysisData]:
        """
        /// è·å–ç”¨æˆ·è¡Œä¸ºåˆ†æ
        /// @param user_id: ç”¨æˆ·ID
        /// @param period: æ—¶é—´å‘¨æœŸ
        /// @return: ç”¨æˆ·åˆ†ææ•°æ®
        """
        try:
            start_date = self._calculate_start_date(period)
            
            async with aiosqlite.connect(self.db_path) as db:
                # ç”¨æˆ·åŸºç¡€æ•°æ®
                cursor = await db.execute(f'''
                    SELECT 
                        COUNT(*) as message_count,
                        AVG(word_count) as avg_length,
                        COUNT(DISTINCT DATE(timestamp)) as active_days
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE user_id = ? AND timestamp >= ?
                ''', (user_id, start_date))
                
                row = await cursor.fetchone()
                if not row or row[0] == 0:
                    return None
                
                message_count, avg_length, active_days = row
                
                # æœ€æ´»è·ƒæ—¶æ®µ
                cursor = await db.execute(f'''
                    SELECT strftime('%H', timestamp) as hour, COUNT(*) as count
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE user_id = ? AND timestamp >= ?
                    GROUP BY hour
                    ORDER BY count DESC
                    LIMIT 1
                ''', (user_id, start_date))
                
                hour_row = await cursor.fetchone()
                most_active_hour = hour_row[0] if hour_row else 'N/A'
                
                # è®¡ç®—å‚ä¸åº¦
                participation_rate = await self._calculate_participation_rate(db, user_id, start_date, message_count, active_days)
                
                # ç”Ÿæˆè¡Œä¸ºæè¿°
                behavior_description = self._generate_behavior_description(
                    message_count, avg_length, active_days, participation_rate
                )
                
                return UserAnalysisData(
                    message_count=message_count,
                    avg_length=avg_length or 0,
                    active_days=active_days,
                    participation_rate=min(100, participation_rate),
                    most_active_hour=most_active_hour,
                    avg_interval='æ­£å¸¸' if message_count > 10 else 'è¾ƒå°‘',
                    behavior_description=behavior_description
                )
                
        except Exception as e:
            logger.error(f"ç”¨æˆ·åˆ†æå¤±è´¥: {e}")
            return None
    
    async def get_topics_analysis(self, group_id: str, period: str) -> Optional[TopicsAnalysisData]:
        """
        /// è·å–è¯é¢˜åˆ†ææ•°æ®
        /// @param group_id: ç¾¤ç»„ID
        /// @param period: æ—¶é—´å‘¨æœŸ
        /// @return: è¯é¢˜åˆ†ææ•°æ®
        """
        try:
            start_date = self._calculate_start_date(period)
            
            async with aiosqlite.connect(self.db_path) as db:
                # è·å–çƒ­é—¨è¯é¢˜
                cursor = await db.execute(f'''
                    SELECT keyword, frequency, last_mentioned
                    FROM {DB.TABLE_TOPIC_KEYWORDS} 
                    WHERE group_id = ? AND last_mentioned >= ?
                    ORDER BY frequency DESC
                    LIMIT 20
                ''', (group_id, start_date))
                
                topics = await cursor.fetchall()
                if not topics:
                    return None
                
                top_topics = [
                    {
                        'keyword': row[0],
                        'frequency': row[1],
                        'last_mentioned': row[2]
                    }
                    for row in topics
                ]
                
                # æ–°è¯é¢˜æ•°é‡
                cursor = await db.execute(f'''
                    SELECT COUNT(DISTINCT keyword)
                    FROM {DB.TABLE_TOPIC_KEYWORDS} 
                    WHERE group_id = ? AND created_at >= ?
                ''', (group_id, start_date))
                
                new_topics_count = (await cursor.fetchone())[0]
                
                # è¯é¢˜æ´»è·ƒåº¦
                total_keywords = len(topics)
                active_topics = len([t for t in top_topics if t['frequency'] > 2])
                topic_activity = (active_topics / max(1, total_keywords)) * 100
                
                # è®¨è®ºæ·±åº¦
                discussion_depth = sum(t['frequency'] for t in top_topics) / max(1, len(top_topics))
                
                return TopicsAnalysisData(
                    top_topics=top_topics,
                    new_topics_count=new_topics_count,
                    topic_activity=topic_activity,
                    discussion_depth=discussion_depth,
                    category_summary="è¯é¢˜ç±»å‹å¤šæ ·ï¼Œæ¶µç›–æ—¥å¸¸äº¤æµã€å…´è¶£çˆ±å¥½ç­‰å„ä¸ªæ–¹é¢"
                )
                
        except Exception as e:
            logger.error(f"è¯é¢˜åˆ†æå¤±è´¥: {e}")
            return None
    
    def _calculate_start_date(self, period: str) -> datetime:
        """è®¡ç®—å¼€å§‹æ—¥æœŸ"""
        now = datetime.now()
        
        if period == TimePeriod.DAY.value:
            return now.replace(hour=0, minute=0, second=0, microsecond=0)
        elif period == TimePeriod.WEEK.value:
            return now - timedelta(days=7)
        elif period == TimePeriod.MONTH.value:
            return now - timedelta(days=30)
        else:
            # è§£æå¦‚ "3d", "7d" æ ¼å¼
            if period.endswith('d') and period[:-1].isdigit():
                days = int(period[:-1])
                return now - timedelta(days=days)
            else:
                return now - timedelta(days=7)  # é»˜è®¤ä¸€å‘¨
    
    def _calculate_growth_rate(self, daily_data: List[Tuple]) -> float:
        """è®¡ç®—å¢é•¿ç‡"""
        if len(daily_data) < 2:
            return 0.0
        
        # æ¯”è¾ƒæœ€è¿‘3å¤©å’Œå‰3å¤©çš„å¹³å‡å€¼
        recent_avg = sum(row[1] for row in daily_data[-3:]) / min(3, len(daily_data))
        early_avg = sum(row[1] for row in daily_data[:3]) / min(3, len(daily_data))
        
        if early_avg > 0:
            return ((recent_avg - early_avg) / early_avg) * 100
        return 0.0
    
    def _generate_trend_description(self, growth_rate: float) -> str:
        """ç”Ÿæˆè¶‹åŠ¿æè¿°"""
        if growth_rate > 10:
            return "ğŸ“ˆ ç¾¤ç»„æ´»è·ƒåº¦å‘ˆä¸Šå‡è¶‹åŠ¿"
        elif growth_rate < -10:
            return "ğŸ“‰ ç¾¤ç»„æ´»è·ƒåº¦æœ‰æ‰€ä¸‹é™"
        else:
            return "ğŸ“Š ç¾¤ç»„æ´»è·ƒåº¦ä¿æŒç¨³å®š"
    
    async def _calculate_participation_rate(self, db: aiosqlite.Connection, user_id: str, 
                                          start_date: datetime, message_count: int, active_days: int) -> float:
        """è®¡ç®—å‚ä¸åº¦"""
        try:
            # è·å–ç¾¤ç»„å¹³å‡å€¼
            cursor = await db.execute(f'''
                SELECT AVG(daily_messages) as group_avg
                FROM (
                    SELECT COUNT(*) as daily_messages
                    FROM {DB.TABLE_MESSAGES} 
                    WHERE group_id = (
                        SELECT group_id FROM {DB.TABLE_MESSAGES} WHERE user_id = ? LIMIT 1
                    ) AND timestamp >= ?
                    GROUP BY user_id, DATE(timestamp)
                )
            ''', (user_id, start_date))
            
            group_avg_row = await cursor.fetchone()
            group_avg = group_avg_row[0] if group_avg_row and group_avg_row[0] else 1
            
            user_daily_avg = message_count / max(1, active_days)
            return (user_daily_avg / group_avg) * 100
            
        except Exception:
            return 50.0  # é»˜è®¤ä¸­ç­‰å‚ä¸åº¦
    
    def _generate_behavior_description(self, message_count: int, avg_length: float, 
                                     active_days: int, participation_rate: float) -> str:
        """ç”Ÿæˆè¡Œä¸ºæè¿°"""
        if participation_rate > 150:
            return "ğŸŒŸ æ‚¨æ˜¯ç¾¤ç»„ä¸­çš„æ´»è·ƒç”¨æˆ·ï¼Œå‘è¨€é¢‘ç‡è¾ƒé«˜"
        elif avg_length > 20:
            return "ğŸ“ æ‚¨å€¾å‘äºå‘é€è¾ƒé•¿çš„æ¶ˆæ¯ï¼Œå†…å®¹è¾ƒä¸ºä¸°å¯Œ"
        elif active_days >= 5:
            return "â° æ‚¨ç»å¸¸å‚ä¸ç¾¤ç»„è®¨è®ºï¼Œæ˜¯æ´»è·ƒæˆå‘˜"
        else:
            return "ğŸ‘¤ æ‚¨çš„å‚ä¸åº¦ä¸­ç­‰ï¼Œå¯ä»¥å°è¯•æ›´å¤šäº’åŠ¨"
    
    async def cleanup_old_data(self, retention_days: int):
        """
        /// æ¸…ç†è¿‡æœŸæ•°æ®
        /// @param retention_days: ä¿ç•™å¤©æ•°
        """
        if retention_days <= 0:
            return
            
        cutoff_date = datetime.now() - timedelta(days=retention_days)
        
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # æ¸…ç†æ—§æ¶ˆæ¯
                cursor = await db.execute(f'DELETE FROM {DB.TABLE_MESSAGES} WHERE timestamp < ?', (cutoff_date,))
                deleted_messages = cursor.rowcount
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                await db.execute(f'DELETE FROM {DB.TABLE_ANALYSIS_CACHE} WHERE expires_at < ?', (datetime.now(),))
                
                # æ¸…ç†å­¤ç«‹çš„å…³é”®è¯è®°å½•
                await db.execute(f'DELETE FROM {DB.TABLE_TOPIC_KEYWORDS} WHERE last_mentioned < ?', (cutoff_date,))
                
                await db.commit()
                logger.info(f"å·²æ¸…ç† {retention_days} å¤©å‰çš„æ•°æ®ï¼Œåˆ é™¤æ¶ˆæ¯æ•°: {deleted_messages}")
                
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
    
    async def update_all_stats(self):
        """
        /// æ›´æ–°æ‰€æœ‰ç»Ÿè®¡æ•°æ®
        /// é‡æ–°è®¡ç®—ç”¨æˆ·å’Œç¾¤ç»„çš„ç»Ÿè®¡ä¿¡æ¯
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # æ›´æ–°ç”¨æˆ·ç»Ÿè®¡ä¸­çš„å¹³å‡å­—æ•°
                await db.execute(f'''
                    UPDATE {DB.TABLE_USER_STATS} 
                    SET avg_words_per_msg = total_words * 1.0 / NULLIF(total_messages, 0),
                        updated_at = ?
                    WHERE total_messages > 0
                ''', (datetime.now(),))
                
                # æ›´æ–°æ´»è·ƒå¤©æ•°
                await db.execute(f'''
                    UPDATE {DB.TABLE_USER_STATS} 
                    SET active_days = (
                        SELECT COUNT(DISTINCT DATE(timestamp))
                        FROM {DB.TABLE_MESSAGES}
                        WHERE {DB.TABLE_MESSAGES}.user_id = {DB.TABLE_USER_STATS}.user_id
                    )
                ''')
                
                await db.commit()
                logger.info("ç»Ÿè®¡æ•°æ®æ›´æ–°å®Œæˆ")
                
        except Exception as e:
            logger.error(f"ç»Ÿè®¡æ›´æ–°å¤±è´¥: {e}")
    
    async def get_database_stats(self) -> Dict[str, Any]:
        """
        /// è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
        /// @return: æ•°æ®åº“çŠ¶æ€ä¿¡æ¯
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                stats = {}
                
                # è¡¨è¡Œæ•°ç»Ÿè®¡
                for table in [DB.TABLE_MESSAGES, DB.TABLE_USER_STATS, 
                             DB.TABLE_GROUP_STATS, DB.TABLE_TOPIC_KEYWORDS]:
                    cursor = await db.execute(f'SELECT COUNT(*) FROM {table}')
                    stats[f'{table}_count'] = (await cursor.fetchone())[0]
                
                # æ•°æ®åº“æ–‡ä»¶å¤§å°
                db_path = Path(self.db_path)
                if db_path.exists():
                    stats['db_size_mb'] = db_path.stat().st_size / (1024 * 1024)
                
                return stats
                
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    # ==================== Phase 2 æ–°å¢ï¼šè¯äº‘å†å²ç®¡ç† ====================
    
    async def save_wordcloud_history(
        self,
        group_id: str,
        time_range: str,
        word_data: Dict[str, int],
        style_name: str,
        file_path: str = None,
        metadata: Dict = None
    ) -> int:
        """
        ä¿å­˜è¯äº‘å†å²è®°å½•
        """
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    INSERT INTO wordcloud_history 
                    (group_id, time_range, word_data, style_name, total_words, file_path, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    group_id,
                    time_range,
                    json.dumps(word_data, ensure_ascii=False),
                    style_name,
                    len(word_data),
                    file_path,
                    json.dumps(metadata or {}, ensure_ascii=False)
                ))
                
                await db.commit()
                record_id = cursor.lastrowid
                logger.info(f"è¯äº‘å†å²è®°å½•å·²ä¿å­˜: {record_id}")
                return record_id
                
        except Exception as e:
            logger.error(f"ä¿å­˜è¯äº‘å†å²å¤±è´¥: {e}")
            return None
    
    async def get_wordcloud_history(
        self,
        group_id: str,
        limit: int = 10,
        time_range: str = None
    ) -> List[Dict]:
        """è·å–è¯äº‘å†å²è®°å½•"""
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                if time_range:
                    cursor = await db.execute("""
                        SELECT * FROM wordcloud_history 
                        WHERE group_id = ? AND time_range = ?
                        ORDER BY generated_at DESC 
                        LIMIT ?
                    """, (group_id, time_range, limit))
                else:
                    cursor = await db.execute("""
                        SELECT * FROM wordcloud_history 
                        WHERE group_id = ?
                        ORDER BY generated_at DESC 
                        LIMIT ?
                    """, (group_id, limit))
                
                rows = await cursor.fetchall()
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                history = []
                for row in rows:
                    record = {
                        'id': row[0],
                        'group_id': row[1],
                        'time_range': row[2],
                        'word_data': json.loads(row[3]),
                        'style_name': row[4],
                        'total_words': row[5],
                        'file_path': row[6],
                        'metadata': json.loads(row[7]) if row[7] else {},
                        'generated_at': row[8],
                        'created_at': row[9]
                    }
                    history.append(record)
                
                return history
                
        except Exception as e:
            logger.error(f"è·å–è¯äº‘å†å²å¤±è´¥: {e}")
            return []
    
    async def compare_wordcloud_history(
        self,
        group_id: str,
        current_data: Dict[str, int],
        days_back: int = 7
    ) -> Dict[str, Any]:
        """å¯¹æ¯”è¯äº‘å†å²æ•°æ®"""
        try:
            import json
            from datetime import datetime, timedelta
            
            # è·å–å†å²æ•°æ®
            compare_date = datetime.now() - timedelta(days=days_back)
            
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    SELECT word_data, generated_at FROM wordcloud_history 
                    WHERE group_id = ? AND generated_at <= ?
                    ORDER BY generated_at DESC 
                    LIMIT 1
                """, (group_id, compare_date.isoformat()))
                
                row = await cursor.fetchone()
                
                if not row:
                    return {
                        'comparison_available': False,
                        'message': f'æ²¡æœ‰æ‰¾åˆ°{days_back}å¤©å‰çš„è¯äº‘æ•°æ®'
                    }
                
                historical_data = json.loads(row[0])
                historical_date = row[1]
                
                # è¿›è¡Œå¯¹æ¯”åˆ†æ
                comparison = self._analyze_wordcloud_changes(
                    current_data, historical_data, historical_date
                )
                
                return {
                    'comparison_available': True,
                    'historical_date': historical_date,
                    'days_compared': days_back,
                    **comparison
                }
                
        except Exception as e:
            logger.error(f"è¯äº‘å†å²å¯¹æ¯”å¤±è´¥: {e}")
            return {'comparison_available': False, 'error': str(e)}
    
    def _analyze_wordcloud_changes(
        self, 
        current_data: Dict[str, int], 
        historical_data: Dict[str, int],
        historical_date: str
    ) -> Dict[str, Any]:
        """åˆ†æè¯äº‘æ•°æ®å˜åŒ–"""
        # æ–°å¢è¯æ±‡
        new_words = set(current_data.keys()) - set(historical_data.keys())
        
        # æ¶ˆå¤±è¯æ±‡
        disappeared_words = set(historical_data.keys()) - set(current_data.keys())
        
        # é¢‘ç‡å˜åŒ–
        frequency_changes = {}
        for word in set(current_data.keys()) & set(historical_data.keys()):
            current_freq = current_data[word]
            historical_freq = historical_data[word]
            change = current_freq - historical_freq
            if change != 0:
                frequency_changes[word] = {
                    'current': current_freq,
                    'historical': historical_freq,
                    'change': change,
                    'change_percent': (change / historical_freq) * 100 if historical_freq > 0 else 100
                }
        
        # çƒ­åº¦ä¸Šå‡è¯æ±‡
        rising_words = sorted(
            [(word, data['change']) for word, data in frequency_changes.items() if data['change'] > 0],
            key=lambda x: x[1], reverse=True
        )[:5]
        
        # çƒ­åº¦ä¸‹é™è¯æ±‡
        falling_words = sorted(
            [(word, data['change']) for word, data in frequency_changes.items() if data['change'] < 0],
            key=lambda x: x[1]
        )[:5]
        
        return {
            'new_words': list(new_words)[:10],
            'disappeared_words': list(disappeared_words)[:10],
            'rising_words': rising_words,
            'falling_words': falling_words,
            'total_current_words': len(current_data),
            'total_historical_words': len(historical_data),
            'word_growth': len(current_data) - len(historical_data),
            'frequency_changes_count': len(frequency_changes)
        }
    
    # ==================== Phase 3 æ–°å¢ï¼šç”¨æˆ·ç”»åƒæ•°æ®ç®¡ç† ====================
    
    async def save_user_portrait(
        self,
        user_portrait
    ) -> Optional[int]:
        """
        ä¿å­˜ç”¨æˆ·ç”»åƒåˆ°æ•°æ®åº“
        
        Args:
            user_portrait: UserPortrait å¯¹è±¡
            
        Returns:
            ç”»åƒè®°å½•ID
        """
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                # ä½¿ç”¨ REPLACE æ¥æ›´æ–°æˆ–æ’å…¥
                cursor = await db.execute("""
                    REPLACE INTO user_portraits 
                    (user_id, group_id, nickname, portrait_data, analysis_depth, 
                     data_quality_score, analysis_duration, generated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    user_portrait.user_id,
                    user_portrait.group_id,
                    user_portrait.nickname,
                    json.dumps(user_portrait.to_dict(), ensure_ascii=False),
                    user_portrait.analysis_depth,
                    user_portrait.data_quality_score,
                    user_portrait.analysis_duration,
                    user_portrait.analysis_date.isoformat()
                ))
                
                await db.commit()
                record_id = cursor.lastrowid
                
                logger.info(f"ç”¨æˆ·ç”»åƒå·²ä¿å­˜: {user_portrait.user_id} (ID: {record_id})")
                return record_id
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return None
    
    async def get_user_portrait(
        self,
        user_id: str,
        group_id: str,
        analysis_depth: str = None
    ) -> Optional[Dict[str, Any]]:
        """
        è·å–ç”¨æˆ·ç”»åƒ
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            analysis_depth: åˆ†ææ·±åº¦ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”¨æˆ·ç”»åƒæ•°æ®
        """
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                if analysis_depth:
                    cursor = await db.execute("""
                        SELECT portrait_data, generated_at FROM user_portraits 
                        WHERE user_id = ? AND group_id = ? AND analysis_depth = ?
                        ORDER BY generated_at DESC 
                        LIMIT 1
                    """, (user_id, group_id, analysis_depth))
                else:
                    cursor = await db.execute("""
                        SELECT portrait_data, generated_at FROM user_portraits 
                        WHERE user_id = ? AND group_id = ?
                        ORDER BY generated_at DESC 
                        LIMIT 1
                    """, (user_id, group_id))
                
                row = await cursor.fetchone()
                
                if row:
                    portrait_data = json.loads(row[0])
                    return portrait_data
                
                return None
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ç”»åƒå¤±è´¥: {e}")
            return None
    
    async def get_user_portrait_history(
        self,
        user_id: str,
        group_id: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        è·å–ç”¨æˆ·ç”»åƒå†å²è®°å½•
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            limit: è®°å½•æ•°é‡é™åˆ¶
            
        Returns:
            ç”»åƒå†å²è®°å½•åˆ—è¡¨
        """
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    SELECT analysis_type, analysis_depth, result_summary, 
                           file_paths, metadata, analysis_time, generated_at
                    FROM portrait_analysis_history 
                    WHERE user_id = ? AND group_id = ?
                    ORDER BY generated_at DESC 
                    LIMIT ?
                """, (user_id, group_id, limit))
                
                rows = await cursor.fetchall()
                
                history = []
                for row in rows:
                    record = {
                        'analysis_type': row[0],
                        'analysis_depth': row[1],
                        'result_summary': row[2],
                        'file_paths': json.loads(row[3]) if row[3] else [],
                        'metadata': json.loads(row[4]) if row[4] else {},
                        'analysis_time': row[5],
                        'generated_at': row[6]
                    }
                    history.append(record)
                
                return history
                
        except Exception as e:
            logger.error(f"è·å–ç”¨æˆ·ç”»åƒå†å²å¤±è´¥: {e}")
            return []
    
    async def save_portrait_analysis_history(
        self,
        user_id: str,
        group_id: str,
        analysis_type: str,
        analysis_depth: str,
        result_summary: str = None,
        file_paths: List[str] = None,
        metadata: Dict[str, Any] = None,
        analysis_time: float = None
    ) -> Optional[int]:
        """
        ä¿å­˜ç”»åƒåˆ†æå†å²è®°å½•
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            analysis_type: åˆ†æç±»å‹ ('portrait', 'comparison')
            analysis_depth: åˆ†ææ·±åº¦
            result_summary: ç»“æœæ‘˜è¦
            file_paths: ç›¸å…³æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata: å…ƒæ•°æ®
            analysis_time: åˆ†æè€—æ—¶
            
        Returns:
            å†å²è®°å½•ID
        """
        try:
            import json
            
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    INSERT INTO portrait_analysis_history 
                    (user_id, group_id, analysis_type, analysis_depth, 
                     result_summary, file_paths, metadata, analysis_time)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    user_id,
                    group_id,
                    analysis_type,
                    analysis_depth,
                    result_summary,
                    json.dumps(file_paths or [], ensure_ascii=False),
                    json.dumps(metadata or {}, ensure_ascii=False),
                    analysis_time
                ))
                
                await db.commit()
                record_id = cursor.lastrowid
                
                logger.info(f"ç”»åƒåˆ†æå†å²å·²ä¿å­˜: {user_id} (ID: {record_id})")
                return record_id
                
        except Exception as e:
            logger.error(f"ä¿å­˜ç”»åƒåˆ†æå†å²å¤±è´¥: {e}")
            return None
    
    async def get_group_portrait_statistics(
        self,
        group_id: str
    ) -> Dict[str, Any]:
        """
        è·å–ç¾¤ç»„ç”»åƒç»Ÿè®¡ä¿¡æ¯
        
        Args:
            group_id: ç¾¤ç»„ID
            
        Returns:
            ç¾¤ç»„ç”»åƒç»Ÿè®¡æ•°æ®
        """
        try:
            async with aiosqlite.connect(self.db_path) as db:
                # ç»Ÿè®¡ç”¨æˆ·ç”»åƒæ•°é‡
                cursor = await db.execute("""
                    SELECT COUNT(DISTINCT user_id) as unique_users,
                           COUNT(*) as total_portraits,
                           analysis_depth,
                           AVG(data_quality_score) as avg_quality,
                           AVG(analysis_duration) as avg_duration
                    FROM user_portraits 
                    WHERE group_id = ?
                    GROUP BY analysis_depth
                """, (group_id,))
                
                portrait_stats = await cursor.fetchall()
                
                # ç»Ÿè®¡åˆ†æå†å²
                cursor = await db.execute("""
                    SELECT analysis_type, COUNT(*) as count,
                           AVG(analysis_time) as avg_time
                    FROM portrait_analysis_history 
                    WHERE group_id = ?
                    GROUP BY analysis_type
                """, (group_id,))
                
                history_stats = await cursor.fetchall()
                
                # æœ€è¿‘æ´»åŠ¨
                cursor = await db.execute("""
                    SELECT COUNT(*) as recent_analyses
                    FROM portrait_analysis_history 
                    WHERE group_id = ? AND generated_at > datetime('now', '-7 days')
                """, (group_id,))
                
                recent_activity = await cursor.fetchone()
                
                return {
                    'portrait_statistics': [
                        {
                            'unique_users': row[0],
                            'total_portraits': row[1],
                            'analysis_depth': row[2],
                            'avg_quality_score': row[3],
                            'avg_duration': row[4]
                        }
                        for row in portrait_stats
                    ],
                    'analysis_history': [
                        {
                            'analysis_type': row[0],
                            'count': row[1],
                            'avg_time': row[2]
                        }
                        for row in history_stats
                    ],
                    'recent_activity': {
                        'analyses_last_7_days': recent_activity[0] if recent_activity else 0
                    }
                }
                
        except Exception as e:
            logger.error(f"è·å–ç¾¤ç»„ç”»åƒç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    async def cleanup_old_portraits(self, days_to_keep: int = 30):
        """
        æ¸…ç†æ—§çš„ç”¨æˆ·ç”»åƒè®°å½•
        
        Args:
            days_to_keep: ä¿ç•™å¤šå°‘å¤©çš„è®°å½•
        """
        try:
            from datetime import datetime, timedelta
            
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            async with aiosqlite.connect(self.db_path) as db:
                # æ¸…ç†ç”¨æˆ·ç”»åƒ
                cursor = await db.execute("""
                    DELETE FROM user_portraits 
                    WHERE generated_at < ?
                """, (cutoff_date.isoformat(),))
                
                portraits_deleted = cursor.rowcount
                
                # æ¸…ç†åˆ†æå†å²
                cursor = await db.execute("""
                    DELETE FROM portrait_analysis_history 
                    WHERE generated_at < ?
                """, (cutoff_date.isoformat(),))
                
                history_deleted = cursor.rowcount
                
                await db.commit()
                
                if portraits_deleted > 0 or history_deleted > 0:
                    logger.info(f"å·²æ¸…ç†ç”¨æˆ·ç”»åƒ: {portraits_deleted} æ¡ï¼Œåˆ†æå†å²: {history_deleted} æ¡")
                
        except Exception as e:
            logger.error(f"æ¸…ç†ç”¨æˆ·ç”»åƒè®°å½•å¤±è´¥: {e}")
    
    async def get_portrait_cache_key(
        self,
        user_id: str,
        group_id: str,
        analysis_depth: str,
        days_back: int
    ) -> str:
        """
        ç”Ÿæˆç”»åƒç¼“å­˜é”®
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            analysis_depth: åˆ†ææ·±åº¦
            days_back: åˆ†æå¤©æ•°
            
        Returns:
            ç¼“å­˜é”®
        """
        return f"portrait_{user_id}_{group_id}_{analysis_depth}_{days_back}"
    
    async def is_portrait_cache_valid(
        self,
        user_id: str,
        group_id: str,
        analysis_depth: str,
        cache_ttl_hours: int = 24
    ) -> bool:
        """
        æ£€æŸ¥ç”»åƒç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            analysis_depth: åˆ†ææ·±åº¦
            cache_ttl_hours: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
            
        Returns:
            ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            from datetime import datetime, timedelta
            
            async with aiosqlite.connect(self.db_path) as db:
                cursor = await db.execute("""
                    SELECT generated_at FROM user_portraits 
                    WHERE user_id = ? AND group_id = ? AND analysis_depth = ?
                    ORDER BY generated_at DESC 
                    LIMIT 1
                """, (user_id, group_id, analysis_depth))
                
                row = await cursor.fetchone()
                
                if row:
                    last_generated = datetime.fromisoformat(row[0])
                    cache_cutoff = datetime.now() - timedelta(hours=cache_ttl_hours)
                    return last_generated > cache_cutoff
                
                return False
                
        except Exception as e:
            logger.error(f"æ£€æŸ¥ç”»åƒç¼“å­˜æœ‰æ•ˆæ€§å¤±è´¥: {e}")
            return False

    async def close(self):
        """
        /// å…³é—­æ•°æ®åº“è¿æ¥
        /// æ¸…ç†èµ„æº
        """
        # aiosqlite æ¯æ¬¡æ“ä½œéƒ½ä¼šè‡ªåŠ¨ç®¡ç†è¿æ¥ï¼Œæ— éœ€æ˜¾å¼å…³é—­
        self.is_initialized = False
        logger.info("æ•°æ®åº“ç®¡ç†å™¨å·²å…³é—­")
