"""
æ™ºèƒ½ç”¨æˆ·ç”»åƒåˆ†æå™¨

åŸºäº LLM çš„ç”¨æˆ·æ€§æ ¼åˆ†æç³»ç»Ÿï¼Œå‚è€ƒ Portrayal è®¾è®¡ç†å¿µ
æä¾›å¤šç»´åº¦ç”¨æˆ·ç‰¹å¾æå–å’Œæ™ºèƒ½æ€§æ ¼åˆ†æåŠŸèƒ½
"""

import json
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import statistics
import asyncio

from astrbot.api import logger
from .models import PluginConfig
from .database import DatabaseManager


class AnalysisDepth(Enum):
    """åˆ†ææ·±åº¦æšä¸¾"""
    LIGHT = "light"        # è½»é‡çº§ï¼šåŸºç¡€ç»Ÿè®¡ + ç®€å•æ ‡ç­¾
    NORMAL = "normal"      # æ ‡å‡†çº§ï¼šè¡Œä¸ºåˆ†æ + LLM æ€§æ ¼åˆ†æ
    DEEP = "deep"          # æ·±åº¦çº§ï¼šå…¨é¢åˆ†æ + è¯¦ç»†æŠ¥å‘Š


class CommunicationStyle(Enum):
    """äº¤æµé£æ ¼æšä¸¾"""
    TALKATIVE = "è¯ç—¨"      # æ¶ˆæ¯é¢‘ç¹ï¼Œå­—æ•°å¤š
    CONCISE = "æƒœå­—å¦‚é‡‘"    # æ¶ˆæ¯å°‘ï¼Œå­—æ•°å°‘
    NORMAL = "æ­£å¸¸äº¤æµ"     # é€‚ä¸­äº¤æµ
    LURKER = "æ½œæ°´å…š"       # å¾ˆå°‘å‘è¨€
    EXPLOSIVE = "çˆ†å‘å‹"    # å¶å°”å¤§é‡å‘è¨€


@dataclass
class UserPortrait:
    """ç”¨æˆ·ç”»åƒæ•°æ®æ¨¡å‹"""
    user_id: str
    group_id: str
    nickname: str
    analysis_date: datetime
    analysis_depth: str
    
    # åŸºç¡€ç»Ÿè®¡æ•°æ®
    message_count: int
    word_count: int
    active_days: int
    avg_words_per_message: float
    active_hours_count: int
    
    # æ—¶é—´è¡Œä¸ºç‰¹å¾
    activity_pattern: Dict[str, float]  # 24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ
    peak_hours: List[int]  # ä¸»è¦æ´»è·ƒæ—¶æ®µ
    weekend_activity: float  # å‘¨æœ«æ´»è·ƒåº¦æ¯”ä¾‹
    
    # äº¤æµç‰¹å¾
    communication_style: str
    favorite_topics: List[str]  # å¸¸ç”¨è¯æ±‡/è¯é¢˜
    message_length_variance: float  # æ¶ˆæ¯é•¿åº¦æ–¹å·®
    
    # LLM åˆ†æç»“æœ (normal/deep çº§åˆ«)
    personality_analysis: Optional[str] = None
    personality_tags: Optional[List[str]] = None
    emotion_tendency: Optional[str] = None
    social_traits: Optional[List[str]] = None
    
    # ç¤¾äº¤å½±å“åŠ› (deep çº§åˆ«)
    interaction_score: Optional[float] = None
    influence_score: Optional[float] = None
    response_rate: Optional[float] = None
    
    # å…ƒæ•°æ®
    analysis_duration: Optional[float] = None
    data_quality_score: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return asdict(self)
    
    def to_summary_text(self) -> str:
        """ç”Ÿæˆç®€è¦æ–‡æœ¬æ€»ç»“"""
        summary = f"""ğŸ“‹ **{self.nickname}** çš„ç”¨æˆ·ç”»åƒ

ğŸ“Š **åŸºç¡€æ•°æ®**
â€¢ å‘è¨€æ¬¡æ•°: {self.message_count:,} æ¡
â€¢ æ€»å­—æ•°: {self.word_count:,} å­—
â€¢ æ´»è·ƒå¤©æ•°: {self.active_days} å¤©
â€¢ å¹³å‡å­—æ•°: {self.avg_words_per_message:.1f} å­—/æ¡

ğŸ•’ **æ´»è·ƒç‰¹å¾**
â€¢ äº¤æµé£æ ¼: {self.communication_style}
â€¢ ä¸»è¦æ´»è·ƒæ—¶æ®µ: {', '.join([f'{h}:00' for h in self.peak_hours[:3]])}
â€¢ å‘¨æœ«æ´»è·ƒåº¦: {self.weekend_activity:.1%}

ğŸ’¬ **è¯é¢˜åå¥½**
â€¢ å¸¸ç”¨è¯æ±‡: {', '.join(self.favorite_topics[:5])}"""

        if self.personality_analysis:
            summary += f"\n\nğŸ§  **æ€§æ ¼åˆ†æ**\n{self.personality_analysis}"
            
        if self.personality_tags:
            summary += f"\n\nğŸ·ï¸ **æ€§æ ¼æ ‡ç­¾**\n{' â€¢ '.join(self.personality_tags)}"
            
        if self.emotion_tendency:
            summary += f"\n\nğŸ˜Š **æƒ…æ„Ÿå€¾å‘**\n{self.emotion_tendency}"
            
        summary += f"\n\nğŸ“… åˆ†ææ—¶é—´: {self.analysis_date.strftime('%Y-%m-%d %H:%M')}"
        summary += f" | ğŸ“ˆ æ•°æ®è´¨é‡: {self.data_quality_score:.1%}" if self.data_quality_score else ""
        
        return summary


class UserPortraitAnalyzer:
    """
    æ™ºèƒ½ç”¨æˆ·ç”»åƒåˆ†æå™¨
    
    åŠŸèƒ½ç‰¹è‰²ï¼š
    - å¤šç»´åº¦ç”¨æˆ·è¡Œä¸ºåˆ†æ
    - LLM é©±åŠ¨çš„æ€§æ ¼åˆ†æ
    - å¯é…ç½®çš„åˆ†ææ·±åº¦
    - æ™ºèƒ½æ•°æ®è´¨é‡è¯„ä¼°
    - æ‰¹é‡ç”¨æˆ·å¯¹æ¯”åˆ†æ
    """
    
    def __init__(self, db_manager: DatabaseManager, config: PluginConfig):
        """
        åˆå§‹åŒ–ç”¨æˆ·ç”»åƒåˆ†æå™¨
        
        Args:
            db_manager: æ•°æ®åº“ç®¡ç†å™¨
            config: æ’ä»¶é…ç½®
        """
        self.db_manager = db_manager
        self.config = config
        
        # LLM é…ç½®
        self.llm_api = None  # å°†åœ¨éœ€è¦æ—¶åˆå§‹åŒ–
        self.max_llm_retries = 3
        self.llm_timeout = 30
        
        # åˆ†æé…ç½®
        self.min_messages_for_analysis = 10  # æœ€å°‘æ¶ˆæ¯æ•°é‡
        self.max_messages_per_analysis = 500  # å•æ¬¡åˆ†ææœ€å¤§æ¶ˆæ¯æ•°
        self.activity_pattern_days = 30  # æ´»è·ƒæ¨¡å¼åˆ†æå¤©æ•°
        
        # ç¼“å­˜
        self.analysis_cache: Dict[str, UserPortrait] = {}
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜
        
        logger.info("ç”¨æˆ·ç”»åƒåˆ†æå™¨å·²åˆå§‹åŒ–")
    
    async def generate_user_portrait(
        self,
        user_id: str,
        group_id: str,
        analysis_depth: AnalysisDepth = AnalysisDepth.NORMAL,
        days_back: int = 30
    ) -> Optional[UserPortrait]:
        """
        ç”Ÿæˆç”¨æˆ·ç”»åƒ
        
        Args:
            user_id: ç”¨æˆ·ID
            group_id: ç¾¤ç»„ID
            analysis_depth: åˆ†ææ·±åº¦
            days_back: åˆ†æå¤šå°‘å¤©å†…çš„æ•°æ®
            
        Returns:
            ç”¨æˆ·ç”»åƒå¯¹è±¡
        """
        start_time = time.time()
        
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{user_id}_{group_id}_{analysis_depth.value}_{days_back}"
            if cache_key in self.analysis_cache:
                cached_portrait = self.analysis_cache[cache_key]
                # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
                if (datetime.now() - cached_portrait.analysis_date).seconds < self.cache_ttl:
                    logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„ç”¨æˆ·ç”»åƒ: {user_id}")
                    return cached_portrait
            
            # è·å–ç”¨æˆ·æ•°æ®
            user_data = await self._collect_user_data(user_id, group_id, days_back)
            if not user_data:
                logger.warning(f"ç”¨æˆ· {user_id} æ•°æ®ä¸è¶³ï¼Œæ— æ³•ç”Ÿæˆç”»åƒ")
                return None
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            quality_score = self._assess_data_quality(user_data)
            if quality_score < 0.3:
                logger.warning(f"ç”¨æˆ· {user_id} æ•°æ®è´¨é‡è¿‡ä½: {quality_score:.2f}")
                return None
            
            # åŸºç¡€ç»Ÿè®¡åˆ†æ
            basic_stats = self._analyze_basic_statistics(user_data)
            
            # è¡Œä¸ºæ¨¡å¼åˆ†æ
            behavior_analysis = self._analyze_behavior_patterns(user_data)
            
            # äº¤æµç‰¹å¾åˆ†æ
            communication_analysis = self._analyze_communication_style(user_data)
            
            # è¯é¢˜åå¥½åˆ†æ
            topic_analysis = self._analyze_topic_preferences(user_data)
            
            # åˆ›å»ºåŸºç¡€ç”»åƒ
            portrait = UserPortrait(
                user_id=user_id,
                group_id=group_id,
                nickname=user_data.get('nickname', user_id),
                analysis_date=datetime.now(),
                analysis_depth=analysis_depth.value,
                **basic_stats,
                **behavior_analysis,
                **communication_analysis,
                favorite_topics=topic_analysis,
                data_quality_score=quality_score,
                analysis_duration=time.time() - start_time
            )
            
            # æ ¹æ®åˆ†ææ·±åº¦è¿›è¡Œé«˜çº§åˆ†æ
            if analysis_depth in [AnalysisDepth.NORMAL, AnalysisDepth.DEEP]:
                await self._perform_llm_analysis(portrait, user_data)
            
            if analysis_depth == AnalysisDepth.DEEP:
                await self._perform_deep_analysis(portrait, user_data)
            
            # ç¼“å­˜ç»“æœ
            self.analysis_cache[cache_key] = portrait
            
            logger.info(f"ç”¨æˆ·ç”»åƒç”Ÿæˆå®Œæˆ: {user_id}, è€—æ—¶: {time.time() - start_time:.2f}s")
            return portrait
            
        except Exception as e:
            logger.error(f"ç”¨æˆ·ç”»åƒç”Ÿæˆå¤±è´¥: {user_id}, é”™è¯¯: {e}")
            return None
    
    async def _collect_user_data(
        self,
        user_id: str,
        group_id: str,
        days_back: int
    ) -> Optional[Dict[str, Any]]:
        """æ”¶é›†ç”¨æˆ·æ•°æ®"""
        try:
            import aiosqlite
            from datetime import datetime, timedelta
            
            start_date = datetime.now() - timedelta(days=days_back)
            
            async with aiosqlite.connect(self.db_manager.db_path) as db:
                # è·å–åŸºç¡€æ¶ˆæ¯æ•°æ®
                cursor = await db.execute("""
                    SELECT content, timestamp, word_count, nickname
                    FROM messages 
                    WHERE user_id = ? AND group_id = ? AND timestamp > ?
                    ORDER BY timestamp DESC
                    LIMIT ?
                """, (user_id, group_id, start_date.isoformat(), self.max_messages_per_analysis))
                
                messages = await cursor.fetchall()
                
                if len(messages) < self.min_messages_for_analysis:
                    return None
                
                # å¤„ç†æ¶ˆæ¯æ•°æ®
                processed_messages = []
                total_words = 0
                nickname = user_id
                
                for msg in messages:
                    content, timestamp, word_count, nick = msg
                    if nick:
                        nickname = nick
                    
                    processed_messages.append({
                        'content': content,
                        'timestamp': datetime.fromisoformat(timestamp),
                        'word_count': word_count or len(content),
                        'hour': datetime.fromisoformat(timestamp).hour,
                        'weekday': datetime.fromisoformat(timestamp).weekday()
                    })
                    total_words += (word_count or len(content))
                
                # è·å–æ´»è·ƒå¤©æ•°
                cursor = await db.execute("""
                    SELECT COUNT(DISTINCT DATE(timestamp)) as active_days
                    FROM messages 
                    WHERE user_id = ? AND group_id = ? AND timestamp > ?
                """, (user_id, group_id, start_date.isoformat()))
                
                active_days_result = await cursor.fetchone()
                active_days = active_days_result[0] if active_days_result else 0
                
                return {
                    'messages': processed_messages,
                    'total_messages': len(messages),
                    'total_words': total_words,
                    'active_days': active_days,
                    'nickname': nickname,
                    'analysis_period_days': days_back
                }
                
        except Exception as e:
            logger.error(f"æ”¶é›†ç”¨æˆ·æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _assess_data_quality(self, user_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        try:
            messages = user_data['messages']
            total_messages = len(messages)
            active_days = user_data['active_days']
            period_days = user_data['analysis_period_days']
            
            # è¯„ä¼°ç»´åº¦
            scores = []
            
            # 1. æ¶ˆæ¯æ•°é‡å……è¶³æ€§ (0-1)
            message_score = min(total_messages / 50, 1.0)  # 50æ¡æ¶ˆæ¯ä¸ºæ»¡åˆ†
            scores.append(message_score)
            
            # 2. æ´»è·ƒå¤©æ•°è¿ç»­æ€§ (0-1)
            activity_score = min(active_days / (period_days * 0.3), 1.0)  # 30%æ´»è·ƒå¤©æ•°ä¸ºæ»¡åˆ†
            scores.append(activity_score)
            
            # 3. æ¶ˆæ¯å†…å®¹ä¸°å¯Œæ€§ (0-1)
            if messages:
                avg_words = sum(msg['word_count'] for msg in messages) / len(messages)
                content_score = min(avg_words / 20, 1.0)  # å¹³å‡20å­—ä¸ºæ»¡åˆ†
                scores.append(content_score)
            
            # 4. æ—¶é—´åˆ†å¸ƒå‡åŒ€æ€§ (0-1)
            if len(messages) > 5:
                hours = [msg['hour'] for msg in messages]
                unique_hours = len(set(hours))
                time_score = min(unique_hours / 12, 1.0)  # 12ä¸ªä¸åŒå°æ—¶ä¸ºæ»¡åˆ†
                scores.append(time_score)
            
            # ç»¼åˆè¯„åˆ†
            quality_score = sum(scores) / len(scores)
            return quality_score
            
        except Exception as e:
            logger.error(f"æ•°æ®è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
            return 0.3  # é»˜è®¤ä¸­ç­‰è´¨é‡
    
    def _analyze_basic_statistics(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æåŸºç¡€ç»Ÿè®¡æ•°æ®"""
        messages = user_data['messages']
        total_messages = len(messages)
        total_words = user_data['total_words']
        active_days = user_data['active_days']
        
        # è®¡ç®—å¹³å‡å­—æ•°
        avg_words_per_message = total_words / total_messages if total_messages > 0 else 0
        
        # è®¡ç®—æ´»è·ƒå°æ—¶æ•°
        active_hours = set(msg['hour'] for msg in messages)
        active_hours_count = len(active_hours)
        
        return {
            'message_count': total_messages,
            'word_count': total_words,
            'active_days': active_days,
            'avg_words_per_message': avg_words_per_message,
            'active_hours_count': active_hours_count
        }
    
    def _analyze_behavior_patterns(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æè¡Œä¸ºæ¨¡å¼"""
        messages = user_data['messages']
        
        # 24å°æ—¶æ´»è·ƒåº¦åˆ†å¸ƒ
        hour_counts = {}
        weekday_counts = {}
        
        for msg in messages:
            hour = msg['hour']
            weekday = msg['weekday']
            
            hour_counts[hour] = hour_counts.get(hour, 0) + 1
            weekday_counts[weekday] = weekday_counts.get(weekday, 0) + 1
        
        # æ ‡å‡†åŒ–ä¸ºæ¯”ä¾‹
        total_messages = len(messages)
        activity_pattern = {
            str(hour): count / total_messages 
            for hour, count in hour_counts.items()
        }
        
        # æ‰¾å‡ºä¸»è¦æ´»è·ƒæ—¶æ®µ (å‰3ä¸ª)
        peak_hours = sorted(hour_counts.keys(), key=lambda h: hour_counts[h], reverse=True)[:3]
        
        # è®¡ç®—å‘¨æœ«æ´»è·ƒåº¦æ¯”ä¾‹
        weekend_messages = weekday_counts.get(5, 0) + weekday_counts.get(6, 0)  # å‘¨å…­å‘¨æ—¥
        weekend_activity = weekend_messages / total_messages if total_messages > 0 else 0
        
        return {
            'activity_pattern': activity_pattern,
            'peak_hours': peak_hours,
            'weekend_activity': weekend_activity
        }
    
    def _analyze_communication_style(self, user_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æäº¤æµé£æ ¼"""
        messages = user_data['messages']
        total_messages = len(messages)
        active_days = user_data['active_days']
        
        if total_messages == 0 or active_days == 0:
            return {'communication_style': CommunicationStyle.LURKER.value, 'message_length_variance': 0}
        
        # è®¡ç®—æ¶ˆæ¯é¢‘ç‡ (æ¡/å¤©)
        message_frequency = total_messages / active_days
        
        # è®¡ç®—å¹³å‡å­—æ•°å’Œæ–¹å·®
        word_counts = [msg['word_count'] for msg in messages]
        avg_words = statistics.mean(word_counts)
        word_variance = statistics.variance(word_counts) if len(word_counts) > 1 else 0
        
        # åˆ¤æ–­äº¤æµé£æ ¼
        style = CommunicationStyle.NORMAL.value
        
        if message_frequency < 1:
            style = CommunicationStyle.LURKER.value
        elif message_frequency > 10 and avg_words > 15:
            style = CommunicationStyle.TALKATIVE.value
        elif avg_words < 5 and message_frequency < 5:
            style = CommunicationStyle.CONCISE.value
        elif word_variance > 100:  # æ¶ˆæ¯é•¿åº¦å˜åŒ–å¾ˆå¤§
            style = CommunicationStyle.EXPLOSIVE.value
        
        return {
            'communication_style': style,
            'message_length_variance': word_variance
        }
    
    def _analyze_topic_preferences(self, user_data: Dict[str, Any]) -> List[str]:
        """åˆ†æè¯é¢˜åå¥½"""
        try:
            import jieba
            from collections import Counter
            
            messages = user_data['messages']
            
            # åˆå¹¶æ‰€æœ‰æ¶ˆæ¯å†…å®¹
            all_content = ' '.join(msg['content'] for msg in messages)
            
            # åˆ†è¯
            words = jieba.lcut(all_content)
            
            # è¿‡æ»¤æ‰åœç”¨è¯å’ŒçŸ­è¯
            filtered_words = [
                word for word in words 
                if len(word) >= 2 and word not in self._get_stop_words()
            ]
            
            # ç»Ÿè®¡è¯é¢‘
            word_freq = Counter(filtered_words)
            
            # è¿”å›å‰10ä¸ªé«˜é¢‘è¯
            return [word for word, _ in word_freq.most_common(10)]
            
        except Exception as e:
            logger.error(f"è¯é¢˜åå¥½åˆ†æå¤±è´¥: {e}")
            return []
    
    def _get_stop_words(self) -> set:
        """è·å–åœç”¨è¯åˆ—è¡¨"""
        return {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
            'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹',
            'å¥½', 'è‡ªå·±', 'è¿™', 'è¿˜', 'ç°åœ¨', 'å¯ä»¥', 'ä»€ä¹ˆ', 'å‡ºæ¥', 'å°±æ˜¯', 'æ—¶å€™',
            'å“ˆå“ˆ', 'å—¯', 'å‘ƒ', 'é¢', 'è¿™æ ·', 'é‚£ä¸ª', 'é‚£ç§', 'è¿™ä¸ª', 'å’‹', 'å•Š',
            'å“¦', 'å—¯å—¯', 'å¥½çš„', 'å¯ä»¥', 'ä½†æ˜¯', 'ä¸è¿‡', 'ç„¶å', 'å› ä¸º', 'æ‰€ä»¥',
            'å¦‚æœ', 'è™½ç„¶', 'è™½ç„¶', 'ä½†', 'å§', 'å‘¢', 'å‘€', 'å“Ÿ', 'å–”', 'å“‡'
        }
    
    async def _perform_llm_analysis(self, portrait: UserPortrait, user_data: Dict[str, Any]):
        """æ‰§è¡Œ LLM æ€§æ ¼åˆ†æ"""
        try:
            # å‡†å¤‡åˆ†ææ•°æ®
            analysis_context = self._prepare_llm_context(portrait, user_data)
            
            # è°ƒç”¨ LLM åˆ†æ
            llm_result = await self._call_llm_for_analysis(analysis_context)
            
            if llm_result:
                portrait.personality_analysis = llm_result.get('personality_analysis')
                portrait.personality_tags = llm_result.get('personality_tags', [])
                portrait.emotion_tendency = llm_result.get('emotion_tendency')
                portrait.social_traits = llm_result.get('social_traits', [])
                
                logger.info(f"LLM åˆ†æå®Œæˆ: {portrait.user_id}")
            else:
                # LLM åˆ†æå¤±è´¥æ—¶çš„é™çº§æ–¹æ¡ˆ
                portrait.personality_analysis = self._generate_rule_based_analysis(portrait)
                portrait.personality_tags = self._generate_rule_based_tags(portrait)
                portrait.emotion_tendency = "åŸºäºè¡Œä¸ºæ¨¡å¼çš„æƒ…æ„Ÿåˆ†ææš‚ä¸å¯ç”¨"
                
        except Exception as e:
            logger.error(f"LLM åˆ†æå¤±è´¥: {e}")
            # ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†æä½œä¸ºé™çº§æ–¹æ¡ˆ
            portrait.personality_analysis = self._generate_rule_based_analysis(portrait)
            portrait.personality_tags = self._generate_rule_based_tags(portrait)
    
    def _prepare_llm_context(self, portrait: UserPortrait, user_data: Dict[str, Any]) -> str:
        """å‡†å¤‡ LLM åˆ†æä¸Šä¸‹æ–‡"""
        messages = user_data['messages']
        
        # é€‰æ‹©ä»£è¡¨æ€§æ¶ˆæ¯ (é¿å…è¿‡é•¿)
        sample_messages = messages[:20] if len(messages) > 20 else messages
        message_samples = [msg['content'] for msg in sample_messages]
        
        context = f"""è¯·åˆ†æç”¨æˆ· "{portrait.nickname}" çš„æ€§æ ¼ç‰¹å¾ï¼š

åŸºç¡€æ•°æ®ï¼š
- å‘è¨€æ¬¡æ•°: {portrait.message_count} æ¡
- æ€»å­—æ•°: {portrait.word_count} å­—
- æ´»è·ƒå¤©æ•°: {portrait.active_days} å¤©
- å¹³å‡å­—æ•°: {portrait.avg_words_per_message:.1f} å­—/æ¡
- äº¤æµé£æ ¼: {portrait.communication_style}
- ä¸»è¦æ´»è·ƒæ—¶æ®µ: {', '.join([f'{h}:00' for h in portrait.peak_hours])}
- å¸¸ç”¨è¯æ±‡: {', '.join(portrait.favorite_topics[:5])}

æ¶ˆæ¯æ ·æœ¬ï¼ˆæœ€è¿‘{len(message_samples)}æ¡ï¼‰ï¼š
{chr(10).join([f'"{msg}"' for msg in message_samples[:10]])}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œä»ä»¥ä¸‹ç»´åº¦åˆ†æè¯¥ç”¨æˆ·ï¼š
1. æ€§æ ¼ç‰¹å¾ï¼ˆ200å­—ä»¥å†…ï¼‰
2. æ€§æ ¼æ ‡ç­¾ï¼ˆ3-5ä¸ªå…³é”®è¯ï¼‰
3. æƒ…æ„Ÿå€¾å‘ï¼ˆç§¯æ/ä¸­æ€§/æ¶ˆæï¼Œç®€è¦è¯´æ˜ï¼‰
4. ç¤¾äº¤ç‰¹è´¨ï¼ˆ2-3ä¸ªç‰¹ç‚¹ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "personality_analysis": "è¯¦ç»†æ€§æ ¼åˆ†æ...",
    "personality_tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "emotion_tendency": "æƒ…æ„Ÿå€¾å‘åˆ†æ...",
    "social_traits": ["ç¤¾äº¤ç‰¹è´¨1", "ç¤¾äº¤ç‰¹è´¨2"]
}}"""
        
        return context
    
    async def _call_llm_for_analysis(self, context: str) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨ LLM è¿›è¡Œåˆ†æ"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„ LLM API
            # ç”±äºæ²¡æœ‰å…·ä½“çš„ LLM é›†æˆï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            
            # æ¨¡æ‹Ÿ LLM åˆ†æå»¶è¿Ÿ
            await asyncio.sleep(0.5)
            
            # è¿”å›æ¨¡æ‹Ÿçš„åˆ†æç»“æœ
            mock_result = {
                "personality_analysis": "è¯¥ç”¨æˆ·å±•ç°å‡ºç§¯ææ´»è·ƒçš„äº¤æµç‰¹å¾ï¼Œå–„äºè¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ï¼Œå…·æœ‰è¾ƒå¼ºçš„ç¤¾äº¤æ„æ„¿ã€‚ä»å‘è¨€é¢‘ç‡å’Œå†…å®¹æ¥çœ‹ï¼Œå±äºå¤–å‘å‹æ€§æ ¼ï¼Œå–œæ¬¢å‚ä¸ç¾¤ä½“è®¨è®ºï¼Œæ€ç»´æ´»è·ƒï¼Œè¡¨è¾¾èƒ½åŠ›è¾ƒå¼ºã€‚",
                "personality_tags": ["æ´»è·ƒ", "å¥è°ˆ", "å¤–å‘", "å‹å–„"],
                "emotion_tendency": "æ•´ä½“æƒ…æ„Ÿå€¾å‘ç§¯æï¼Œåœ¨ç¾¤èŠä¸­è¡¨ç°å‡ºç§¯æå‚ä¸çš„æ€åº¦ï¼Œè¾ƒå°‘å‡ºç°è´Ÿé¢æƒ…ç»ªè¡¨è¾¾ã€‚",
                "social_traits": ["ä¹äºåˆ†äº«", "ç§¯æäº’åŠ¨", "ç¾¤ä½“èå…¥æ„Ÿå¼º"]
            }
            
            logger.info("LLM åˆ†æè°ƒç”¨æˆåŠŸ (æ¨¡æ‹Ÿ)")
            return mock_result
            
        except Exception as e:
            logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
            return None
    
    def _generate_rule_based_analysis(self, portrait: UserPortrait) -> str:
        """ç”ŸæˆåŸºäºè§„åˆ™çš„æ€§æ ¼åˆ†æï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        analysis_parts = []
        
        # åŸºäºäº¤æµé£æ ¼çš„åˆ†æ
        style_analysis = {
            CommunicationStyle.TALKATIVE.value: "è¯¥ç”¨æˆ·è¡¨ç°å‡ºæ´»è·ƒçš„äº¤æµç‰¹å¾ï¼Œå–„äºè¡¨è¾¾ï¼Œå–œæ¬¢åˆ†äº«ï¼Œå…·æœ‰è¾ƒå¼ºçš„ç¤¾äº¤å€¾å‘ã€‚",
            CommunicationStyle.CONCISE.value: "è¯¥ç”¨æˆ·åå¥½ç®€æ´çš„è¡¨è¾¾æ–¹å¼ï¼Œè¨€ç®€æ„èµ…ï¼Œå¯èƒ½æ›´æ³¨é‡æ•ˆç‡æˆ–è¾ƒä¸ºå†…æ•›ã€‚",
            CommunicationStyle.LURKER.value: "è¯¥ç”¨æˆ·è¾ƒå°‘ä¸»åŠ¨å‘è¨€ï¼Œå¯èƒ½æ›´å€¾å‘äºè§‚å¯Ÿå’Œå€¾å¬ï¼Œæˆ–åœ¨ç¾¤ä½“ä¸­è¾ƒä¸ºä½è°ƒã€‚",
            CommunicationStyle.EXPLOSIVE.value: "è¯¥ç”¨æˆ·å‘è¨€æ¨¡å¼å¤šå˜ï¼Œåœ¨æŸäº›æ—¶å€™ä¼šæœ‰å¤§é‡è¡¨è¾¾ï¼Œæ˜¾ç¤ºå‡ºæƒ…ç»ªåŒ–çš„äº¤æµç‰¹å¾ã€‚",
            CommunicationStyle.NORMAL.value: "è¯¥ç”¨æˆ·ä¿æŒé€‚åº¦çš„äº¤æµé¢‘ç‡ï¼Œè¡¨ç°å‡ºå¹³è¡¡çš„ç¤¾äº¤ç‰¹å¾ã€‚"
        }
        
        analysis_parts.append(style_analysis.get(portrait.communication_style, "ç”¨æˆ·äº¤æµç‰¹å¾æ­£å¸¸ã€‚"))
        
        # åŸºäºæ´»è·ƒæ—¶é—´çš„åˆ†æ
        if portrait.peak_hours:
            peak_hour = portrait.peak_hours[0]
            if 6 <= peak_hour <= 8:
                analysis_parts.append("ä¸»è¦åœ¨æ—©æ™¨æ—¶æ®µæ´»è·ƒï¼Œå¯èƒ½æ˜¯æ—©èµ·å‹äººæ ¼ã€‚")
            elif 12 <= peak_hour <= 14:
                analysis_parts.append("åˆé—´æ—¶æ®µè¾ƒä¸ºæ´»è·ƒï¼Œå±•ç°å‡ºè§„å¾‹çš„ä½œæ¯ä¹ æƒ¯ã€‚")
            elif 18 <= peak_hour <= 22:
                analysis_parts.append("æ™šé—´æ—¶æ®µæœ€ä¸ºæ´»è·ƒï¼Œå¯èƒ½æ›´å€¾å‘äºåœ¨ä¼‘æ¯æ—¶é—´è¿›è¡Œç¤¾äº¤ã€‚")
            elif 22 <= peak_hour or peak_hour <= 2:
                analysis_parts.append("æ·±å¤œæ—¶æ®µæ´»è·ƒï¼Œå¯èƒ½æ˜¯å¤œçŒ«å­å‹äººæ ¼ã€‚")
        
        # åŸºäºå‘¨æœ«æ´»è·ƒåº¦çš„åˆ†æ
        if portrait.weekend_activity > 0.4:
            analysis_parts.append("å‘¨æœ«æ´»è·ƒåº¦è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºè¾ƒå¼ºçš„ä¼‘é—²ç¤¾äº¤å€¾å‘ã€‚")
        elif portrait.weekend_activity < 0.2:
            analysis_parts.append("å·¥ä½œæ—¥æ›´ä¸ºæ´»è·ƒï¼Œå¯èƒ½æ›´æ³¨é‡æ—¥å¸¸äº¤æµã€‚")
        
        return " ".join(analysis_parts)
    
    def _generate_rule_based_tags(self, portrait: UserPortrait) -> List[str]:
        """ç”ŸæˆåŸºäºè§„åˆ™çš„æ€§æ ¼æ ‡ç­¾ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        tags = []
        
        # åŸºäºäº¤æµé£æ ¼
        style_tags = {
            CommunicationStyle.TALKATIVE.value: ["å¥è°ˆ", "æ´»è·ƒ", "å¤–å‘"],
            CommunicationStyle.CONCISE.value: ["ç®€æ´", "é«˜æ•ˆ", "å†…æ•›"],
            CommunicationStyle.LURKER.value: ["ä½è°ƒ", "è§‚å¯Ÿå‹", "è°¨æ…"],
            CommunicationStyle.EXPLOSIVE.value: ["æƒ…ç»ªåŒ–", "å¤šå˜", "è¡¨è¾¾å¼ºçƒˆ"],
            CommunicationStyle.NORMAL.value: ["å¹³è¡¡", "ç¨³å®š", "é€‚åº¦"]
        }
        
        tags.extend(style_tags.get(portrait.communication_style, ["æ­£å¸¸"]))
        
        # åŸºäºæ´»è·ƒæ—¶é—´
        if portrait.peak_hours:
            peak_hour = portrait.peak_hours[0]
            if 6 <= peak_hour <= 8:
                tags.append("æ—©èµ·å‹")
            elif 22 <= peak_hour or peak_hour <= 2:
                tags.append("å¤œçŒ«å­")
        
        # åŸºäºæ¶ˆæ¯æ•°é‡
        if portrait.message_count > 100:
            tags.append("æ´»è·ƒç”¨æˆ·")
        elif portrait.message_count < 20:
            tags.append("å¶å°”å‘è¨€")
        
        return tags[:5]  # æœ€å¤šè¿”å›5ä¸ªæ ‡ç­¾
    
    async def _perform_deep_analysis(self, portrait: UserPortrait, user_data: Dict[str, Any]):
        """æ‰§è¡Œæ·±åº¦åˆ†æ"""
        try:
            messages = user_data['messages']
            
            # è®¡ç®—äº’åŠ¨å¾—åˆ† (æ¨¡æ‹Ÿ)
            interaction_score = min(portrait.message_count / 100, 1.0)
            
            # è®¡ç®—å½±å“åŠ›å¾—åˆ† (åŸºäºæ¶ˆæ¯é•¿åº¦å’Œé¢‘ç‡)
            avg_words = portrait.avg_words_per_message
            influence_score = min((avg_words * portrait.message_count) / 1000, 1.0)
            
            # è®¡ç®—å›å¤ç‡ (æ¨¡æ‹Ÿ - å®é™…éœ€è¦åˆ†æå›å¤å…³ç³»)
            response_rate = 0.8  # é»˜è®¤å€¼
            
            portrait.interaction_score = interaction_score
            portrait.influence_score = influence_score
            portrait.response_rate = response_rate
            
            logger.info(f"æ·±åº¦åˆ†æå®Œæˆ: {portrait.user_id}")
            
        except Exception as e:
            logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")
    
    async def compare_users(
        self,
        user1_id: str,
        user2_id: str,
        group_id: str,
        days_back: int = 30
    ) -> Optional[Dict[str, Any]]:
        """
        å¯¹æ¯”ä¸¤ä¸ªç”¨æˆ·çš„ç”»åƒ
        
        Args:
            user1_id: ç”¨æˆ·1 ID
            user2_id: ç”¨æˆ·2 ID
            group_id: ç¾¤ç»„ID
            days_back: åˆ†æå¤©æ•°
            
        Returns:
            å¯¹æ¯”åˆ†æç»“æœ
        """
        try:
            # ç”Ÿæˆä¸¤ä¸ªç”¨æˆ·çš„ç”»åƒ
            portrait1 = await self.generate_user_portrait(user1_id, group_id, AnalysisDepth.NORMAL, days_back)
            portrait2 = await self.generate_user_portrait(user2_id, group_id, AnalysisDepth.NORMAL, days_back)
            
            if not portrait1 or not portrait2:
                return None
            
            # è¿›è¡Œå¯¹æ¯”åˆ†æ
            comparison = {
                'user1': portrait1.to_dict(),
                'user2': portrait2.to_dict(),
                'comparison_summary': self._generate_comparison_summary(portrait1, portrait2),
                'similarity_score': self._calculate_similarity_score(portrait1, portrait2),
                'differences': self._identify_key_differences(portrait1, portrait2)
            }
            
            return comparison
            
        except Exception as e:
            logger.error(f"ç”¨æˆ·å¯¹æ¯”å¤±è´¥: {e}")
            return None
    
    def _generate_comparison_summary(self, p1: UserPortrait, p2: UserPortrait) -> str:
        """ç”Ÿæˆå¯¹æ¯”æ‘˜è¦"""
        summary_parts = []
        
        # æ´»è·ƒåº¦å¯¹æ¯”
        if p1.message_count > p2.message_count * 1.5:
            summary_parts.append(f"{p1.nickname} æ¯” {p2.nickname} æ›´åŠ æ´»è·ƒ")
        elif p2.message_count > p1.message_count * 1.5:
            summary_parts.append(f"{p2.nickname} æ¯” {p1.nickname} æ›´åŠ æ´»è·ƒ")
        else:
            summary_parts.append("ä¸¤äººæ´»è·ƒåº¦ç›¸è¿‘")
        
        # äº¤æµé£æ ¼å¯¹æ¯”
        if p1.communication_style != p2.communication_style:
            summary_parts.append(f"äº¤æµé£æ ¼ä¸åŒï¼š{p1.nickname}({p1.communication_style}) vs {p2.nickname}({p2.communication_style})")
        
        # æ´»è·ƒæ—¶é—´å¯¹æ¯”
        if set(p1.peak_hours) & set(p2.peak_hours):
            summary_parts.append("åœ¨æŸäº›æ—¶æ®µéƒ½æ¯”è¾ƒæ´»è·ƒ")
        else:
            summary_parts.append("æ´»è·ƒæ—¶é—´æ®µä¸åŒ")
        
        return "ï¼›".join(summary_parts)
    
    def _calculate_similarity_score(self, p1: UserPortrait, p2: UserPortrait) -> float:
        """è®¡ç®—ç›¸ä¼¼åº¦å¾—åˆ†"""
        scores = []
        
        # äº¤æµé£æ ¼ç›¸ä¼¼åº¦
        style_score = 1.0 if p1.communication_style == p2.communication_style else 0.0
        scores.append(style_score)
        
        # æ´»è·ƒæ—¶é—´ç›¸ä¼¼åº¦
        common_hours = len(set(p1.peak_hours) & set(p2.peak_hours))
        time_score = common_hours / max(len(p1.peak_hours), len(p2.peak_hours), 1)
        scores.append(time_score)
        
        # è¯é¢˜ç›¸ä¼¼åº¦
        common_topics = len(set(p1.favorite_topics) & set(p2.favorite_topics))
        topic_score = common_topics / max(len(p1.favorite_topics), len(p2.favorite_topics), 1)
        scores.append(topic_score)
        
        return sum(scores) / len(scores)
    
    def _identify_key_differences(self, p1: UserPortrait, p2: UserPortrait) -> List[str]:
        """è¯†åˆ«å…³é”®å·®å¼‚"""
        differences = []
        
        # æ¶ˆæ¯æ•°é‡å·®å¼‚
        msg_ratio = max(p1.message_count, p2.message_count) / max(min(p1.message_count, p2.message_count), 1)
        if msg_ratio > 2:
            more_active = p1.nickname if p1.message_count > p2.message_count else p2.nickname
            differences.append(f"{more_active} å‘è¨€æ˜æ˜¾æ›´é¢‘ç¹")
        
        # å­—æ•°å·®å¼‚
        if abs(p1.avg_words_per_message - p2.avg_words_per_message) > 10:
            longer_msg = p1.nickname if p1.avg_words_per_message > p2.avg_words_per_message else p2.nickname
            differences.append(f"{longer_msg} å¹³å‡æ¶ˆæ¯æ›´é•¿")
        
        # å‘¨æœ«æ´»è·ƒåº¦å·®å¼‚
        if abs(p1.weekend_activity - p2.weekend_activity) > 0.3:
            weekend_person = p1.nickname if p1.weekend_activity > p2.weekend_activity else p2.nickname
            differences.append(f"{weekend_person} å‘¨æœ«æ›´æ´»è·ƒ")
        
        return differences
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """è·å–åˆ†æç»Ÿè®¡ä¿¡æ¯"""
        return {
            'cached_portraits': len(self.analysis_cache),
            'cache_ttl': self.cache_ttl,
            'min_messages_threshold': self.min_messages_for_analysis,
            'max_messages_per_analysis': self.max_messages_per_analysis,
            'llm_timeout': self.llm_timeout,
            'max_retries': self.max_llm_retries
        }
